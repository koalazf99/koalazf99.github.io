---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from:
  - /about/
  - /about.html
---


<span class='anchor' id='about-me'></span>

<link href='https://fonts.googleapis.com/css?family=Turret+Road:400,600,400italic,600italic,300,300italic'
    rel='stylesheet' type='text/css'>
# ğŸ¨ About Me
I am **Fan Zhou**, currently a 1st year PhD student at **[Shanghai Jiao Tong University](https://en.sjtu.edu.cn/)**, advised by Prof. **[Pengfei Liu](http://pfliu.com/)**.
My research focuses on scalable methods (data, toolkits, and recipes) for building performant models, with the aim of contributing to powerful general-purpose AI *(or AGI if you would like to call it)* . 
Recently, Iâ€™ve been particularly interested in the following areas:
1. Developing Data-Centric Recipes for Foundation Models \\
  ([**GURU**](https://www.arxiv.org/abs/2506.14965), [**OctoThinker**](https://natural-rugby-f7c.notion.site/OctoThinker-Revisiting-Mid-Training-1d20b810e2d680c494a9f9dad0a90d53), [**MegaMath**](https://arxiv.org/abs/2504.02807), [**ProX**](https://arxiv.org/abs/2409.17115), [**Sailor2**](https://arxiv.org/abs/2502.12982))
2. Building Agentic AI for Real-World Scenarios \\
  ([**Qwen Code**](https://github.com/QwenLM/qwen-code), [**OpenAgents**](https://github.com/xlang-ai/OpenAgents), [**Lemur**](https://arxiv.org/abs/2310.06830))





# ğŸ”¥ News

<style>  
    .scrollable-area {  
        max-height: 190px;  
        overflow-y: auto;  
        box-shadow: 0 2px 12px rgba(0, 0, 0, 0.08);  /* æ›´æŸ”å’Œçš„é˜´å½± */
        padding: 10px 15px;  /* å¢åŠ å†…è¾¹è·è®©å†…å®¹ä¸è´´è¾¹ */
        color: #333;
        border: 1px solid #e0e0e0;  /* æµ…ç°è‰²è¾¹æ¡† */
        border-radius: 8px;  /* åœ†è§’è¾¹æ¡† */
        background-color: #ffffff;  /* ç¡®ä¿èƒŒæ™¯æ˜¯ç™½è‰² */
    }
    /* è®¾ç½®æ»šåŠ¨æ¡çš„å®½åº¦å’Œè½¨é“èƒŒæ™¯ */
    .scrollable-area::-webkit-scrollbar {
        width: 8px;
    }
    /* è®¾ç½®æ»šåŠ¨æ¡è½¨é“ */
    .scrollable-area::-webkit-scrollbar-track {
        background: #f1f1f1;
        border-radius: 4px;
    }
    /* è®¾ç½®æ»šåŠ¨æ¡æ»‘å— */
    .scrollable-area::-webkit-scrollbar-thumb {
        background: #888;
        border-radius: 4px;
    }

    /* é¼ æ ‡æ‚¬åœåœ¨æ»‘å—ä¸Šæ—¶çš„æ ·å¼ */
    .scrollable-area::-webkit-scrollbar-thumb:hover {
        background: #555;
    }
    .pdf {
        text-decoration: none;
        color: #122c8b;
    }
    .code {
        text-decoration: none;
        color: #122c8b;
    }
    .title{
        color: #374798;
    }
</style>

<div class="scrollable-area" style="width:100%;">
    <ul>
        <li><em>2025.07</em>: ğŸ“„ <strong>MegaMath</strong> paper is accepted by COLM'25.</li>
        <li><em>2025.06</em>: ğŸ™‹ We release <strong>GURU</strong>, a large-scale RL Study for general-purpose reasoning models across 6 domains.</li>
        <li><em>2025.05</em>: ğŸ“„ <strong>ProX</strong> and <strong>MSTaR</strong> paper are accepted by ICML'25.</li>
        <li><em>2025.04</em>: ğŸ”¥ Say hi to <strong>OctoThinker</strong>, a mid-training ablation study in the era of RL scaling.</li>
        <li><em>2025.04</em>: ğŸ”¥ We release <strong>MegaMath</strong>, the largest math pre-training dataset to date containing 370B tokens.</li>
        <li><em>2024.12</em>: ğŸ”¥ Enjoy <a href="https://sea-sailor.github.io/blog/sailor2/"><strong>Sailor2</strong></a>, a state-of-the-art language model family for south-east asia.</li>
        <li><em>2024.11</em>: ğŸ”¥ We have released <a href="https://mstar-lmm.github.io/"><strong>MStaR</strong></a>, a self-evolving training recipe for multimodal reasoning.</li>
        <li><em>2024.09</em>: ğŸ”¥ We have released <a href="https://arxiv.org/abs/2409.17115"><strong>ProX</strong></a>, a small-LM-based pre-training data refining framework!</li>
        <li><em>2024.09</em>: ğŸ“„ <strong>OlympicArena</strong> paper is accepted by Neurips'24.</li>
        <li><em>2024.07</em>: ğŸ“„ <strong>OpenAgents</strong> paper is accepted by COLM'24.</li>
        <li><em>2024.05</em>: ğŸ“„ <strong>Preference Dissection</strong> paper is accepted by ACL'24.</li>
        <li><em>2024.01</em>: ğŸ“„ Our <strong>Lemur</strong> paper(Agent Model) is accepted by ICLR'24 (<strong><font color="#cc0000">Spotlight</font></strong>, 5%).</li>  
        <li><em>2023.10</em>: ğŸ”¥ We've built <a href="https://github.com/xlang-ai/OpenAgents">OpenAgents</a>, an open platform for language agents in the wild!</li>  
        <li><em>2023.10</em>: ğŸ™‹ We have released <a href="https://arxiv.org/abs/2310.06830">Lemur-70B</a>, an agentic language model based on LLama-2!</li>  
        <li><em>2023.04</em>: ğŸ”¥ New <a href="https://arxiv.org/abs/2304.07995">preprint</a> applying <strong>symbolic tasks</strong> in <strong>instruction tuning</strong></li>  
        <li><em>2022.10</em>: ğŸ“„ Our <strong>TaCube</strong> paper(Table QA) is accepted by EMNLP'22 (<strong><font color="#cc0000">Oral</font></strong> Presentation).</li>  
    </ul>  
</div>

{% include publications.md %}

# Experiences

<!-- 

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

2017 â”€â”€â”€â”€â”€â”€â–º 2021      2021 â”€â”€â”€â”€â”€â”€â–º 2024      2024 â”€â”€â”€â”€â”€â”€â–º Present
    B.S.                  M.S.                    Ph.D.
    IEEE Honor            Computer               Computer
    Class               Science                Science
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

        ğŸ›ï¸ SJTU           ğŸ›ï¸ SJTU             ğŸ›ï¸ SJTU 
-->

- <img src="images/education.png" width=18em style="vertical-align: middle;"> _2021.09 - 2024.03_, M.S.@SJTU, CS.
- <img src="images/education.png" width=18em style="vertical-align: middle;"> _2017.09 - 2021.06_, B.S.@SJTU, CS, IEEE honor class.

<!-- 
ğŸ¯ Hidden Professional Journey (Easter Egg!)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

     ğŸ¢ MSRA (2021-2022)
        â”‚
        â”œâ”€â–º Data & Knowledge Intelligence
        â””â”€â–º Award of Excellence
        
     ğŸŒŸ XLang-HKU (2023)
        â”‚
        â””â”€â–º Cross-lingual AI Research
        
     ğŸŒŠ Sea AI Lab (2024-present)
        â”‚
        â””â”€â–º Research Collaborator, Singapore
        
     ğŸ§ª Shanghai AI Lab (2024)
        â”‚
        â””â”€â–º Research Assistant
        
     ğŸš€ LLM360 (2024-2025)
        â”‚
        â””â”€â–º Open Source LLM Initiative
        
     âš¡ Qwen Team (2025-present)
        â”‚
        â””â”€â–º Next-Gen Language Models

ASCII Art by Fan Zhou ğŸ¨
-->

# Service and Awards

- Reviewer: ICLR, NeurIPS, COLM, ACL, IJCAI, COLING, ...
- MSRA: Award of Excellent Intern, 2022
<!-- - Outstanding Graduates of SJTU, 2021 -->
<!-- - SJTU Academic Scholarship, 2017~2020 -->
<!-- - Shanghai City Scholarship(â‰ˆtop 5%), 2018 -->